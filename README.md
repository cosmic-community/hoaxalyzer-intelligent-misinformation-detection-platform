# üîç Hoaxalyzer - Intelligent Misinformation Detection Platform

![Hoaxalyzer Preview](https://images.unsplash.com/photo-1504711434969-e33886168f5c?w=1200&h=300&fit=crop&auto=format)

**Hoaxalyzer** adalah platform web canggih yang menganalisis artikel berita dan topik media sosial untuk mendeteksi sentimen publik dan mengklasifikasikan potensi misinformasi/hoax menggunakan state-of-the-art Natural Language Processing (NLP). Dibangun dengan fokus akademis untuk penelitian tesis dengan metodologi yang dapat dipertanggungjawabkan.

## ‚ú® Features

- **üéØ Dual Analysis Mode**: Analisis single URL artikel atau monitoring topik trending
- **ü§ñ Advanced NLP Pipeline**: IndoBERT-based sentiment & hoax classification
- **üí° Explainable AI**: LIME/SHAP integration untuk transparansi model
- **üìä Interactive Dashboards**: Real-time visualizations dengan Chart.js
- **üó∫Ô∏è Geospatial Mapping**: Visualisasi sebaran geografis informasi
- **‚ö° Asynchronous Processing**: Celery task queue untuk analisis berat
- **üîê Secure Architecture**: PostgreSQL + Redis untuk data persistence
- **üì± Responsive Design**: Mobile-first UI dengan Tailwind CSS
- **üåê Multi-Source Scraping**: Automated data collection dari news portals & social media
- **üìà Sentiment Tracking**: Time-series analysis untuk monitoring tren sentimen

## Clone this Project

## Clone this Project

Want to create your own version of this project with all the content and structure? Clone this Cosmic bucket and code repository to get started instantly:

[![Clone this Project](https://img.shields.io/badge/Clone%20this%20Project-29abe2?style=for-the-badge&logo=cosmic&logoColor=white)](https://app.cosmicjs.com/projects/new?clone_bucket=68f5aa664c6c2e246ad69492&clone_repository=68f737b4278bb75aded05543)

## Prompts

This application was built using the following prompts to generate the content structure and code:

### Content Model Prompt

> No content model prompt provided - app built from existing content structure

### Code Generation Prompt

> Design a complete system architecture for 'Hoaxalyzer' - a web portal that analyzes news articles and social media topics to detect public sentiment and classify potential misinformation/hoax. The project should have strong academic weight, suitable for thesis research, with focus on NLP methodology and interactive data visualization. Core tech stack: React TypeScript frontend with D3.js/Chart.js, Tailwind CSS, Python FastAPI backend, PostgreSQL database, Redis caching, Celery for async processing, Hugging Face Transformers with IndoBERT, spaCy, scikit-learn, and web scraping with BeautifulSoup/Scrapy. Features include landing page with methodology explanation, analysis input form (single URL or keyword topic), interactive dashboard with sentiment analysis visuals, hoax probability scores, explainability components, word clouds, geospatial maps, and article/tweet lists. Backend should have async task queue, scraping services, NLP pipeline with preprocessing for Indonesian language, sentiment analysis, hoax classification, and LIME/SHAP explainability. Database schema should support analysis jobs, collected articles, analysis results, and tweets.

The app has been tailored to work with your existing Cosmic content structure and includes all the features requested above.

## üõ†Ô∏è Technologies Used

### Frontend
- **Next.js 15** - React framework with App Router
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling
- **Chart.js** - Interactive data visualizations
- **React Query** - Data fetching & caching

### Backend
- **Python FastAPI** - High-performance async API
- **Celery** - Distributed task queue
- **Redis** - Caching & message broker
- **PostgreSQL** - Primary database

### NLP & ML
- **Hugging Face Transformers** - IndoBERT models
- **spaCy** - Text preprocessing
- **Scikit-learn** - ML utilities
- **LIME/SHAP** - Model explainability

### Data Collection
- **BeautifulSoup4** - HTML parsing
- **Scrapy** - Web scraping framework
- **Playwright** - Browser automation

### CMS
- **Cosmic** - Headless CMS for content management

## üöÄ Getting Started

### Prerequisites

- **Node.js 18+** and **Bun** installed
- **Python 3.10+** with pip
- **PostgreSQL 14+** database
- **Redis 7+** server
- Cosmic account with bucket created

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd hoaxalyzer
```

2. **Install frontend dependencies**
```bash
bun install
```

3. **Set up Python virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
cp .env.example .env
```

Edit `.env` with your credentials:
```env
# Cosmic CMS (automatically configured)
COSMIC_BUCKET_SLUG=your-bucket-slug
COSMIC_READ_KEY=your-read-key
COSMIC_WRITE_KEY=your-write-key

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/hoaxalyzer
REDIS_URL=redis://localhost:6379

# Python Backend API
BACKEND_API_URL=http://localhost:8000

# Twitter/X API (optional - for social media scraping)
TWITTER_BEARER_TOKEN=your-twitter-bearer-token

# Model Configuration
SENTIMENT_MODEL=indobert-sentiment-classifier
HOAX_MODEL=indobert-hoax-detector
```

5. **Initialize database**
```bash
# Run migrations (from backend directory)
cd backend
python scripts/init_db.py
cd ..
```

6. **Download NLP models**
```bash
python backend/scripts/download_models.py
```

### Running the Application

**Terminal 1 - Frontend (Next.js)**
```bash
bun run dev
```

**Terminal 2 - Backend API (FastAPI)**
```bash
cd backend
uvicorn main:app --reload --port 8000
```

**Terminal 3 - Celery Worker**
```bash
cd backend
celery -A celery_worker worker --loglevel=info
```

**Terminal 4 - Redis Server**
```bash
redis-server
```

**Terminal 5 - PostgreSQL**
```bash
# If not running as service
postgres -D /usr/local/var/postgres
```

Visit `http://localhost:3000` to see the application.

## üìö Cosmic SDK Examples

### Storing Analysis Results

```typescript
// lib/cosmic-operations.ts
import { cosmic } from '@/lib/cosmic'

export async function saveAnalysisResult(data: {
  jobId: string
  queryType: 'url' | 'topic'
  queryInput: string
  results: any
}) {
  try {
    const response = await cosmic.objects.insertOne({
      type: 'analysis-results',
      title: `Analysis: ${data.queryInput}`,
      slug: data.jobId,
      metadata: {
        job_id: data.jobId,
        query_type: data.queryType,
        query_input: data.queryInput,
        overall_sentiment: data.results.overallSentiment,
        hoax_probability: data.results.hoaxProbability,
        articles_analyzed: data.results.articlesCount,
        sentiment_breakdown: data.results.sentimentBreakdown,
        top_keywords: data.results.topKeywords,
        analyzed_at: new Date().toISOString()
      }
    })
    
    return response.object
  } catch (error) {
    console.error('Error saving analysis result:', error)
    throw new Error('Failed to save analysis result')
  }
}
```

### Fetching Historical Analysis

```typescript
// app/history/page.tsx
import { cosmic } from '@/lib/cosmic'

export default async function HistoryPage() {
  try {
    const { objects: analyses } = await cosmic.objects
      .find({ type: 'analysis-results' })
      .props(['id', 'title', 'slug', 'metadata', 'created_at'])
      .depth(0)
    
    return (
      <div className="container mx-auto py-8">
        <h1 className="text-3xl font-bold mb-6">Analysis History</h1>
        <div className="grid gap-4">
          {analyses.map(analysis => (
            <AnalysisCard key={analysis.id} analysis={analysis} />
          ))}
        </div>
      </div>
    )
  } catch (error) {
    return <div>No analysis history found</div>
  }
}
```

### Managing Training Datasets

```typescript
// lib/dataset-operations.ts
export async function addTrainingExample(example: {
  text: string
  label: 'hoax' | 'factual'
  source: string
}) {
  try {
    const response = await cosmic.objects.insertOne({
      type: 'training-data',
      title: `Training: ${example.text.substring(0, 50)}...`,
      metadata: {
        text_content: example.text,
        classification_label: example.label,
        source_url: example.source,
        verified: false,
        added_at: new Date().toISOString()
      }
    })
    
    return response.object
  } catch (error) {
    console.error('Error adding training example:', error)
    throw new Error('Failed to add training example')
  }
}
```

## üéì Cosmic CMS Integration

Hoaxalyzer uses Cosmic as a headless CMS to manage:

1. **Analysis Results Archive**
   - Stores completed analysis with full metadata
   - Enables historical trend analysis
   - Supports reproducibility for research

2. **Training Dataset Management**
   - Crowdsourced labeled data for model improvement
   - Version control for dataset iterations
   - Verification workflow for data quality

3. **Research Documentation**
   - Methodology documentation
   - Model performance metrics
   - Academic papers and citations

4. **User Submissions Tracking**
   - Analysis request history
   - User feedback on results
   - Feature requests and bug reports

All data is automatically synced with Cosmic CMS, providing a powerful backend for research and continuous improvement.

## üåê Deployment Options

### Vercel (Recommended for Frontend)

1. Connect your GitHub repository to Vercel
2. Configure environment variables in Vercel dashboard
3. Deploy with automatic CI/CD

### Backend Deployment Options

**Option 1: Railway**
```bash
# Install Railway CLI
npm i -g @railway/cli

# Deploy backend
cd backend
railway login
railway init
railway up
```

**Option 2: Render**
- Create new Web Service
- Connect repository
- Add environment variables
- Deploy Python application

**Option 3: AWS EC2/DigitalOcean**
- Set up Ubuntu server
- Install dependencies
- Configure Nginx reverse proxy
- Set up systemd services for FastAPI & Celery

### Database Hosting

- **PostgreSQL**: Railway, Supabase, or AWS RDS
- **Redis**: Redis Labs, Railway, or AWS ElastiCache

### Environment Variables

Set these in your deployment platform:
```env
COSMIC_BUCKET_SLUG=your-bucket-slug
COSMIC_READ_KEY=your-read-key
COSMIC_WRITE_KEY=your-write-key
DATABASE_URL=your-production-database-url
REDIS_URL=your-production-redis-url
BACKEND_API_URL=your-backend-api-url
```

## üìä Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Frontend (Next.js)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ Landing Page ‚îÇ  ‚îÇ Input Form   ‚îÇ  ‚îÇ  Dashboard   ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Backend API (FastAPI)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ            REST API Endpoints                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  POST /analyze/url    POST /analyze/topic            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  GET /results/{job_id}  GET /status/{job_id}        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Celery Task Queue     ‚îÇ   ‚îÇ    Redis (Broker)       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Analysis Worker  ‚îÇ   ‚îÇ   ‚îÇ  ‚îÇ  Job Queue       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Scraping Worker  ‚îÇ   ‚îÇ   ‚îÇ  ‚îÇ  Result Cache    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              NLP Processing Pipeline                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Scraping ‚îÇ‚Üí ‚îÇPreprocessing‚îÇ‚Üí‚îÇSentiment ‚îÇ‚Üí‚îÇ  Hoax    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ   (spaCy)  ‚îÇ  ‚îÇ(IndoBERT)‚îÇ  ‚îÇClassifier‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                      ‚îÇ                       ‚îÇ
‚îÇ                                      ‚ñº                       ‚îÇ
‚îÇ                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ                            ‚îÇ  Explainability  ‚îÇ             ‚îÇ
‚îÇ                            ‚îÇ   (LIME/SHAP)    ‚îÇ             ‚îÇ
‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               PostgreSQL Database                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇAnalysisJobs  ‚îÇ  ‚îÇ  Articles    ‚îÇ  ‚îÇ   Results    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    Tweets    ‚îÇ  ‚îÇ TrainingData ‚îÇ  ‚îÇ              ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Cosmic CMS                                ‚îÇ
‚îÇ         (Content Management & Research Archive)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
hoaxalyzer/
‚îú‚îÄ‚îÄ app/                          # Next.js App Router
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx               # Root layout
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                 # Landing page
‚îÇ   ‚îú‚îÄ‚îÄ analyze/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Analysis input form
‚îÇ   ‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [jobId]/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ page.tsx        # Dashboard results
‚îÇ   ‚îú‚îÄ‚îÄ history/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Historical analyses
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îî‚îÄ‚îÄ bucket-info/
‚îÇ           ‚îî‚îÄ‚îÄ route.ts        # API route for bucket info
‚îú‚îÄ‚îÄ components/                  # React components
‚îÇ   ‚îú‚îÄ‚îÄ AnalysisForm.tsx
‚îÇ   ‚îú‚îÄ‚îÄ DashboardCard.tsx
‚îÇ   ‚îú‚îÄ‚îÄ SentimentChart.tsx
‚îÇ   ‚îú‚îÄ‚îÄ HoaxScoreCard.tsx
‚îÇ   ‚îú‚îÄ‚îÄ ExplainabilityBox.tsx
‚îÇ   ‚îú‚îÄ‚îÄ WordCloud.tsx
‚îÇ   ‚îú‚îÄ‚îÄ GeospatialMap.tsx
‚îÇ   ‚îú‚îÄ‚îÄ ArticleList.tsx
‚îÇ   ‚îî‚îÄ‚îÄ CosmicBadge.tsx
‚îú‚îÄ‚îÄ lib/                         # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ cosmic.ts               # Cosmic SDK client
‚îÇ   ‚îú‚îÄ‚îÄ api-client.ts           # Backend API client
‚îÇ   ‚îî‚îÄ‚îÄ types.ts                # TypeScript types
‚îú‚îÄ‚îÄ backend/                     # Python backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI app
‚îÇ   ‚îú‚îÄ‚îÄ celery_worker.py        # Celery configuration
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nlp_pipeline.py    # NLP processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentiment_model.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hoax_classifier.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper.py         # Web scraping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ twitter_crawler.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py          # SQLAlchemy models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ crud.py            # Database operations
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îú‚îÄ‚îÄ init_db.py         # Database initialization
‚îÇ       ‚îî‚îÄ‚îÄ download_models.py # Download NLP models
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ dashboard-console-capture.js
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ package.json               # Node dependencies
‚îú‚îÄ‚îÄ tailwind.config.js
‚îú‚îÄ‚îÄ tsconfig.json
‚îî‚îÄ‚îÄ next.config.js
```

---

Built with ‚ù§Ô∏è for academic research and public good. Powered by **Cosmic CMS** for scalable content management.

<!-- README_END -->